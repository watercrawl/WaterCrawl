{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f0b9afa",
   "metadata": {},
   "source": [
    "### ğŸš€ WaterCrawlÂ Ã—Â FLARE â€” the perfect duo for RAG playgrounds!\n",
    "\n",
    "Welcome to this **stepâ€‘byâ€‘step JupyterÂ Notebook** where we:\n",
    "\n",
    "1. ğŸ•· **Crawl & clean** any website with **WaterCrawl** â€“ turning raw HTML into markdown/JSON thatâ€™s ready for embeddings.Â   \n",
    "2. ğŸ” **Retrieve onâ€‘theâ€‘fly** with **FLARE (Forwardâ€‘LookingÂ ActiveÂ REtrieval)** â€“ an â€œalwaysâ€‘beâ€‘factâ€‘checkingâ€ wrapper that pulls extra docs *only* when the LLM shows low confidence.  \n",
    "3. ğŸ›  **Tie it all together** with **LangChain**, **Tavily SearchÂ API** & a few helper utils so you can remix the pipeline to your heartâ€™s content.\n",
    "\n",
    "---\n",
    "\n",
    "#### Whatâ€™s inside?\n",
    "\n",
    "| ğŸ”§ Component | ğŸ’¡ Why weâ€™re using it |\n",
    "|--------------|----------------------|\n",
    "| **WaterCrawl** | Pointâ€‘&â€‘shoot crawling with sitemap visualizer, duplicate detection, and markdown/JSON exports â€“ perfect for vector DB ingestion. :contentReference[oaicite:0]{index=0} |\n",
    "| **LangChain** | Glue layer that lets us chain the crawl â†’ embed â†’ FLARE retrieval steps with a few lines of code. |\n",
    "| **TavilyÂ SearchÂ API** | Fast, inexpensive web search that slots into `TavilyRetriever`; great complement to your own crawled corpora. |\n",
    "| **FLARE** | Reâ€‘checks the modelâ€™s â€œnext sentenceâ€ for shaky tokens; if confidence is low, it autoâ€‘generates a smart query and fetches fresh docs before writing. :contentReference[oaicite:1]{index=1} |\n",
    "\n",
    "---\n",
    "\n",
    "#### Notebook flow ğŸ—ºï¸\n",
    "\n",
    "1. **Setup**: grab your API keys from https://watercrawl.dev/, spin up your own `watercrawl` from: https://github.com/watercrawl/watercrawl. To run WaterCrawl API you need to install the Python SDK, which we will do in the following steps\n",
    "2. **FLARE chain**: initialize `FlareChain(llm_answer, llm_question, retriever)` with **Tavily** + your newlyâ€‘minted vector store.  \n",
    "3. **Ask away!**: watch FLARE pause, retrieve, and resume writingâ€”as many times as neededâ€”to give rockâ€‘solid answers.  \n",
    "4. **Extras**: show off the visual sitemap PNG WaterCrawl generated and link each node to its vector IDs.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why youâ€™ll â¤ï¸ this combo\n",
    "\n",
    "- **Less hallucination, more citation**: WaterCrawl hands FLARE pristine, sourceâ€‘mapped text, so every sentence can be traced back to a URL.  \n",
    "- **Pay only for what you need**: FLARE calls Tavily *selectively*, not on every tokenâ€”so your search bill stays tiny.  \n",
    "- **Dropâ€‘in for any stack**: swap Tavily for your own BM25/Elastic/Weaviate retriever, or point WaterCrawl at authenticated intranet sites.  \n",
    "- **Openâ€‘source all the way**: MITâ€‘style licences on both projects mean you can fork, tweak, and ship to prod. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "> **Tip:** if youâ€™re new to WaterCrawl, follow: https://github.com/watercrawl/watercrawl?tab=readme-ov-file#-quick-start  hit `http://localhost` after `docker compose up -d` and explore the Playground UIâ€”selector testing & screenshot capture included! ğŸ¨\n",
    "\n",
    "---\n",
    "\n",
    "Ready? Letâ€™s spin up containers and start crawling! ğŸ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e41fe0",
   "metadata": {},
   "source": [
    "##### â¡ï¸ **Lets install ğŸ“¦all the dependencies:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f902f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install  langchain-community langchain-core langchain-openai notebook watercrawl-py tavily-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bbe7f",
   "metadata": {},
   "source": [
    "### â¡ï¸ ğŸ”‘ **API keys youâ€™ll need (grab these first!)** \n",
    "\n",
    "| Service | What itâ€™s for | Where to generate |\n",
    "|---------|---------------|-------------------|\n",
    "| **WaterCrawl** | Auth for crawling endpoints | <https://app.watercrawl.dev/dashboard/api-keys> |\n",
    "| **OpenAI** | LLM + embeddings | <https://platform.openai.com/api-keys> |\n",
    "| **Tavily Search** | Web search for FLARE | <https://app.tavily.com/home> |\n",
    "\n",
    "---\n",
    "\n",
    "OptionÂ 1 â€“keep it clean: use a `.env` file âš ï¸\n",
    "\n",
    "\n",
    "Create the file **once**, store your keys, and everything else â€œjust worksâ€.\n",
    "\n",
    "```python\n",
    "# â”€â”€ create_env.py â”€â”€\n",
    "env_text = \"\"\"\n",
    "OPENAI_API_KEY= ***put your APi key here *** \n",
    "TAVILY_API_KEY= ***put your APi key here *** \n",
    "WATERCRAWL_API_KEY=* **put your APi key here *** \n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_text)\n",
    "print(\".env file created â€” now edit it with your real keys âœï¸\")\n",
    "\n",
    "-------------------------------------------------\n",
    "OptionÂ 2 â€“ quickâ€‘andâ€‘dirty: hardâ€‘code in the notebook âš ï¸\n",
    "\n",
    "OPENAI_API_KEY= ***put your APi key here *** \n",
    "TAVILY_API_KEY= ***put your APi key here *** \n",
    "WATERCRAWL_API_KEY=* **put your APi key here *** \n",
    "\n",
    "Not recommended â€” anyone who sees or commits the notebook can read your keys.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac82939",
   "metadata": {},
   "source": [
    "##### â¡ï¸ **If youâ€™re using aÂ `.env`Â file load the API keys with dotenv** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1babea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # pulls everything from .env\n",
    "\n",
    "OPENAI_API_KEY   = os.environ.get(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY   = os.environ.get(\"TAVILY_API_KEY\")\n",
    "WATERCRAWL_API_KEY = os.environ.get(\"WATERCRAWL_API_KEY\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7e4b63d",
   "metadata": {},
   "source": [
    "##### â¡ï¸ **Import our packages**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7888f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForRetrieverRun,\n",
    "    CallbackManagerForRetrieverRun,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from watercrawl import WaterCrawlAPIClient\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import requests\n",
    "from langchain.chains import FlareChain\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f552dce",
   "metadata": {},
   "source": [
    "#### â¡ï¸ **Lets build WaterCrawl Retriever**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43a04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_tool(query: str, api_key: str, max_results: int = 3) -> List[str]:\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"topic\": \"general\",\n",
    "        \"search_depth\": \"basic\",\n",
    "        \"max_results\": max_results,\n",
    "        \"include_answer\": False,\n",
    "        \"include_raw_content\": False,\n",
    "        \"include_domains\": [],\n",
    "        \"exclude_domains\": []\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "    print([item.get(\"url\") for item in results.get(\"results\", []) if item.get(\"url\")])\n",
    "    return [item.get(\"url\") for item in results.get(\"results\", []) if item.get(\"url\")]\n",
    "\n",
    "\n",
    "class WaterCrawlRetriever(BaseRetriever, BaseModel):\n",
    "    client: WaterCrawlAPIClient\n",
    "    tavily_api_key: str\n",
    "    page_options: dict = {\n",
    "        \"exclude_tags\": [\"nav\", \"footer\", \"aside\"],\n",
    "        \"include_tags\": [\"article\", \"main\"],\n",
    "        \"wait_time\": 100,\n",
    "        \"include_html\": False,\n",
    "        \"only_main_content\": True,\n",
    "        \"include_links\": False\n",
    "    }\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, **kwargs: Any\n",
    "    ) -> List[Document]:\n",
    "        documents = []\n",
    "        try:\n",
    "            urls = search_tool(query, self.tavily_api_key, max_results=3)\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    result = self.client.scrape_url(url=url, page_options=self.page_options, sync=True, download=True)\n",
    "                    content = result.get(\"content\", \"\")\n",
    "                    if content:\n",
    "                        documents.append(Document(page_content=content, metadata={\"source\": url}))\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to fetch content from {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Tavily search failed: {e}\")\n",
    "        return documents\n",
    "\n",
    "    async def _aget_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        run_manager: AsyncCallbackManagerForRetrieverRun,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[Document]:\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070d91e",
   "metadata": {},
   "source": [
    "#### â¡ï¸ **Create the langchain retriever obect using WaterCrawlRetriever we have built above**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c7d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = WaterCrawlRetriever(client=WaterCrawlAPIClient(api_key=WATERCRAWL_API_KEY),\n",
    "    tavily_api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92478194",
   "metadata": {},
   "source": [
    "#### â¡ï¸ **FLARE Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577e7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set this so we can see what exactly is going on\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300d783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( model=\"gpt-4o\", temperature=0)\n",
    "flare = FlareChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    max_generation_len=164,\n",
    "    min_prob=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f3d5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain what is watercrawl tool and how I can improve the LLM performance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1bfa8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What type of software is the Watercrawl tool?', 'How can you optimize the data extraction process to improve the LLM performance?']\u001b[0m\n",
      "['https://github.com/watercrawl/watercrawl', 'https://docs.watercrawl.dev/intro', 'https://watercrawl.dev/']\n",
      "['https://techcommunity.microsoft.com/blog/azure-ai-services-blog/maximizing-data-extraction-precision-with-dual-llms-integration-and-human-in-the/4236728', 'https://medium.com/intel-tech/four-data-cleaning-techniques-to-improve-large-language-model-llm-performance-77bee9003625', 'https://www.turing.com/resources/understanding-data-processing-techniques-for-llms']\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response:  The Watercrawl tool is a software tool used for web crawling and data extraction. It helps in collecting data from websites for various purposes such as research, analysis, or monitoring. To improve the LLM (Large Language Model) performance, you can consider optimizing the data collection process using Watercrawl to ensure high-quality and relevant data inputs for training the model. Additionally, you can also focus on fine-tuning the hyperparameters of the LLM and increasing the training data size to enhance its performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'Explain what is watercrawl tool and how I can improve the LLM performance?',\n",
       " 'response': 'The Watercrawl tool is a software tool used for web crawling and data extraction. It helps in collecting data from websites for various purposes such as research, analysis, or monitoring. To improve the LLM (Large Language Model) performance, you can consider optimizing the data collection process using Watercrawl to ensure high-quality and relevant data inputs for training the model. Additionally, you can also focus on fine-tuning the hyperparameters of the LLM and increasing the training data size to enhance its performance. '}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flare.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â¡ï¸ **Now lets see a simple Open AI chain so we can see the value of the FLARE Chain**\n",
    "#### for the test query we provided, the answer of the same LLM is completely wrong!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bed8944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWatercrawl is a web performance testing tool that helps in analyzing the load and stress on a website or web application. It simulates real-world user traffic and measures the website's response time, throughput, and server performance under different load conditions.\\n\\nTo improve the LLM (Load, Latency, and Memory) performance using Watercrawl, the following steps can be taken:\\n\\n1. Identify bottlenecks: Watercrawl helps in identifying the areas of the website that are causing performance issues. It provides detailed reports on page load times, HTTP requests, and server response times, which can help in identifying the bottlenecks.\\n\\n2. Optimize website code: Based on the reports generated by Watercrawl, developers can optimize the website's code to reduce page load times and improve server response times. This can include techniques like minimizing HTTP requests, optimizing images, and using caching mechanisms.\\n\\n3. Test under different load conditions: Watercrawl allows testing under different load conditions, such as low, medium, and high traffic. This helps in identifying how the website performs under different levels of user traffic and if it can handle a sudden surge in traffic.\\n\\n4. Test from different geographical locations: Watercrawl offers the option to test the website's performance from different geographical locations. This helps in identifying\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37b8e9",
   "metadata": {},
   "source": [
    "##### ğŸš¨âš ï¸ As you have noted, for the test query we provided, the answer from the **same LLM** is **completely wrong** âŒğŸ¤¯â€¼ï¸\n",
    "\n",
    "> ğŸ’¬ It confidently gives a **wrong answer** â€” showing **why refinement and retrieval matter** so much in real-world usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4f0f297",
   "metadata": {},
   "source": [
    "#### â¡ï¸ **For further information**:\n",
    "##### ğŸ“˜ Introduction to FlareChain in LangChain\n",
    "\n",
    "**FlareChain** is an advanced chain in the LangChain framework ğŸ§ âš™ï¸ designed to *iteratively refine answers* from a language model. It improves response quality by:\n",
    "\n",
    "ğŸ” Identifying **low-confidence** spans  \n",
    "â“ Generating **clarifying questions**  \n",
    "ğŸ“š Retrieving **relevant context**  \n",
    "ğŸ” Updating the answer in a loop\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§© Key Arguments of `FlareChain`\n",
    "\n",
    "##### ğŸ—£ 2. `response_chain`\n",
    "Generates the actual response using user input + context.\n",
    "\n",
    "##### ğŸ§¾ 3. `output_parser`\n",
    "Checks whether the current answer is â€œgood enoughâ€ to stop refinement.\n",
    "\n",
    "##### ğŸ“¡ 4. `retriever`\n",
    "Fetches documents to provide factual backup for refining the answer.\n",
    "\n",
    "##### ğŸ“‰ 5. `min_prob`\n",
    "Low-confidence threshold (default: `0.2`) â€“ tokens below this are flagged for review.\n",
    "\n",
    "##### â†”ï¸ 6. `min_token_gap`\n",
    "Ensures separation between two flagged spans (default: `5` tokens).\n",
    "\n",
    "##### ğŸ§· 7. `num_pad_tokens`\n",
    "Adds context tokens around flagged spans (default: `2`).\n",
    "\n",
    "##### ğŸ” 8. `max_iter`\n",
    "Max number of refinement cycles (default: `10`).\n",
    "\n",
    "##### ğŸ§­ 9. `start_with_retrieval`\n",
    "If `True`, starts by retrieving context even before generating the first draft.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¾ Inputs and Outputs\n",
    "\n",
    "- ğŸ“¥ **Input Key**: `user_input`  \n",
    "- ğŸ“¤ **Output Key**: `response`  \n",
    "\n",
    "ğŸ’¡ The chain processes a single user prompt and returns an *improved, confident, and context-aware response*.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“š References\n",
    "\n",
    "- [LangChain FlareChain Documentation](https://api.python.langchain.com/en/latest/langchain/chains/langchain.chains.flare.base.FlareChain.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadd022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
